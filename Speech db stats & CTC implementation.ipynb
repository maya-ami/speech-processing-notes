{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech data exploration & CTC Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLKl4jO26g3n"
      },
      "source": [
        "#HarperValleyBank Speech Data Exploration & CTC Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh8A_5L1dq6-"
      },
      "source": [
        "## Setup for Google Drive and Required Libraries\n",
        "\n",
        "You will need to make a copy of this Colab notebook in your Google Drive before you can edit the homework files.\n",
        "\n",
        "You can do so with **File &rarr; Save a copy in Drive**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9ahWfoydgHv",
        "outputId": "c5546ed7-f7fe-4057-e030-26ce84b1b7dd"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/speech_processing_notes'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "SYM_PATH = '/content/speech_processing_notes'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CzA2i4g_Jk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11909e6b-82ca-4ebf-c6cf-1b0fc577068d"
      },
      "source": [
        "#@title Download HarperValleyBank dataset\n",
        "#@markdown It takes ~10 minutes to download the dataset. You only need to do this once!\n",
        "\n",
        "DATA_PATH = '{}/data'.format(SYM_PATH)\n",
        "if not os.path.exists(DATA_PATH):\n",
        "  %mkdir $DATA_PATH\n",
        "%cd $DATA_PATH\n",
        "if not os.path.exists(os.path.join(DATA_PATH, 'harpervalleybank')):\n",
        "  !wget -q http://web.stanford.edu/class/speech_processing_notes/download/harpervalleybank.zip\n",
        "  !unzip -q harpervalleybank.zip\n",
        "  %rm harpervalleybank.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/cs224s/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjUD2H-wyEYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30a2a94-a153-42b9-8c48-55f0db48b70e"
      },
      "source": [
        "#@title Import packages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from scipy.fftpack import fft\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8yumkFH7EVL"
      },
      "source": [
        "## Part 1:  HarperValleyBank Dataset Exploration\n",
        "\n",
        "Let's first explore the [HarperValleyBank](https://arxiv.org/abs/2010.13929) dataset! The dataset primarily consists of simulated telephone/app-based consumer to banker interactions. For any new dataset, it is generally a good idea to explore the \"shape\" and \"properties\" of the data. This will help greatly when debugging unexpected behavior in your speech system.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa4lP4mUem9Y"
      },
      "source": [
        "## Step 1.1 Getting Corpus Statistics [4 Points]\n",
        "\n",
        "For any new dataset, it is generally a good idea to explore the \"shape\" and \"properties\" of the data. This will help greatly when debugging unexpected behavior in your speech system. \n",
        "\n",
        "The structure of the HarperValleyBank dataset is:\n",
        "```\n",
        "data\n",
        "    audio\n",
        "        agent\n",
        "            <sid1>.wav\n",
        "            <sid2>.wav\n",
        "            ...\n",
        "        caller\n",
        "            <sid1>.wav\n",
        "            <sid2>.wav\n",
        "            ...\n",
        "    metadata\n",
        "        <sid1>.json\n",
        "        <sid2>.json\n",
        "        ...\n",
        "    transcript\n",
        "        <sid1>.json\n",
        "        <sid2>.json\n",
        "        ...\n",
        "```\n",
        "Every consumer-banker conversation has an id referred to as it' `sid ` .  All associated files are named based on that sid.\n",
        "Each conversation has four associated files, two audio files, one transcript file and one metadata file.\n",
        "The audio for each conversation is divided in to two single channel wav files, available under the audio/agent and audio/caller directories.\n",
        "\n",
        "##### <ins>**Task 1.1.1**</ins>: **Number of conversations** **(2 points)**\n",
        "\n",
        "Please fill out the function `number_of_conversations` to get the number of conversations in the dataset. (This part could be done with 1 line of code)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1YetAUB1UxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922baafe-c438-49a7-aed7-8ed74c0d3c65"
      },
      "source": [
        "agent_audio_path = '/content/speech_processing_notes/data/harpervalleybank/audio/agent/'\n",
        "caller_audio_path = '/content/speech_processing_notes/data/harpervalleybank/audio/caller/'\n",
        "transcript_path = '/content/speech_processing_notes/data/harpervalleybank/transcript/'\n",
        "metadata_path = '/content/speech_processing_notes/data/harpervalleybank/metadata/'\n",
        "\n",
        "def number_of_conversations(path: str) -> int:\n",
        "  \"\"\"Gets number of conversations in the dataset.\n",
        "\n",
        "  Args:\n",
        "    path: Path to relevant directory.\n",
        "\n",
        "  Returns:\n",
        "    Number of conversations.\n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "  return len(os.listdir(path))\n",
        "  #############################   \n",
        "\n",
        "path = transcript_path \n",
        "print('Number of conversations: ' + str(number_of_conversations(path)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of conversations: 1446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp5LIYu17jAN"
      },
      "source": [
        "##### <ins>**Task 1.1.2</ins>: Plot call duration (2 points)**\n",
        "\n",
        "Please fill out the function `recording_time` to get the duration of recording in seconds and plot a histogram with duration of recording (in seconds) as x-axis and count of conversations as y-axis. You can use `wavfile.read()` method. \n",
        "\n",
        "For a single conversation, there are two audio files: `caller` and `agent` audio files. Both audio files for the same conversation have the same duration of recording because the other person's voice is replaced with silence. Thus, please choose either caller or agent directory to get the total duration or recording. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1QMwt0MRW9-"
      },
      "source": [
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrIXf9Kf7mBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0995a54-0d1f-47f3-bb61-2eb5aae96b00"
      },
      "source": [
        "def recording_time(path_dir: str) -> Tuple[float, np.ndarray]:\n",
        "  \"\"\"Gets duration of recording\n",
        "  Args:\n",
        "    path_dir: Path to relevant directory.\n",
        "\n",
        "  Returns:\n",
        "    Duration of total recording in seconds (float),\n",
        "    Numpy array of duration of recording for all conversations\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "  ind_audio_dur = [int(librosa.get_duration(filename=os.path.join(path_dir, file))) for file in os.listdir(path_dir)]\n",
        "  total_dur = sum(ind_audio_dur)\n",
        "  return (total_dur, np.array(ind_audio_dur))\n",
        "  #############################   \n",
        "\n",
        "path_agent = agent_audio_path \n",
        "path_caller = caller_audio_path \n",
        "\n",
        "duration, ind_durations = recording_time(path_agent)\n",
        "print(\"Total duration of recordings in seconds \" + str(duration))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total duration of recordings in seconds 83410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "elFtvMOzXhLd",
        "outputId": "b7bd384d-42d5-4ab7-dea4-b6048be9cd63"
      },
      "source": [
        "# Plot histogram: X - Duration of recording of individual file, Y - Count of audio files\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "import pandas as pd\n",
        "dur_df = pd.DataFrame({'seconds':ind_durations})\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.countplot(dur_df['seconds'].value_counts(), palette='twilight')\n",
        "#############################"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f75682f3b50>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZh40UnlBgev"
      },
      "source": [
        "## Step 1.2 Transcript Analysis [16 Points]\n",
        "\n",
        "**Transcripts:** The transcript json files are lists of segments where each segment is a json object with the following schema:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"channel_index\": 2,\n",
        "    \"dialog_acts\": [\n",
        "        \"gridspace_greeting\"\n",
        "    ],\n",
        "    \"duration_ms\": 2280,\n",
        "    \"emotion\": {\n",
        "        \"neutral\": 0.33766093850135803,\n",
        "        \"negative\": 0.024230705574154854,\n",
        "        \"positive\": 0.6381083130836487\n",
        "    },\n",
        "    \"human_transcript\": \"hello this is harper valley national bank\",\n",
        "    \"index\": 1,\n",
        "    \"offset_ms\": 5990,\n",
        "    \"speaker_role\": \"agent\",\n",
        "    \"start_ms\": 3990,\n",
        "    \"start_timestamp_ms\": 1591056064136,\n",
        "    \"transcript\": \"hello this is harper valley national bank\",\n",
        "    \"word_durations_ms\": [\n",
        "        330,\n",
        "        150,\n",
        "        120,\n",
        "        330,\n",
        "        270,\n",
        "        420,\n",
        "        330\n",
        "    ],\n",
        "    \"word_offsets_ms\": [\n",
        "        0,\n",
        "        660,\n",
        "        810,\n",
        "        930,\n",
        "        1260,\n",
        "        1530,\n",
        "        1950\n",
        "    ]\n",
        "}\n",
        "```\n",
        "The fields we will closely analyze for inference are:\n",
        "\n",
        "- **\"human_transcript\":** Corrected version of the machine genereated \"transcript\".\n",
        "\n",
        "- **\"emotion\":** Softmax output of Gridspace's Emotion model, determining whether the emotional valence of the segment was positive, negative, or neutral.\n",
        "\n",
        "- **\"dialog_acts\":**. List of tags assigned to each utterance corresponding to types of conversational move represented in the utterance. There are 16 total dialog actions, and more than one can be present in an utterance. The 16 possible actions are: “yes” response, greeting, response, data confirmation, procedure explanation, data question, closing, data communication, “bear with me” response, acknowledgement, data response, filler disfluency, thanks, open question, problem description, and other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW59u7UqBwRo"
      },
      "source": [
        "##### <ins>**Task 1.2.1**</ins>**: Transcript Statistics (2 points)**\n",
        "\n",
        "Load all transcript json files using `json.load()` and fill out the function `transcript_statistics` to get the following statistics: \n",
        "- Total number of utterances\n",
        "- Mean number of agent utterances per conversation\n",
        "- Mean number of caller utterances per conversation\n",
        "- Total number of words: Keep in mind that \"[\", \"]\", \"<\", \">\" aren't considered as words. \n",
        "- Number of unique words \n",
        "\n",
        "You can get the text of the audio file by looking at the **\"human_transcript\"** field for each transcript. You can use `word_tokenize` function from `nltk.tokenize`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S61cpfB5Gn9O"
      },
      "source": [
        "def transcript_statistics(\n",
        "    path_transcript: str) -> Tuple[int, float, float, int, int]:\n",
        "  \"\"\"Get transcript statistics.\n",
        "\n",
        "  Args:\n",
        "    path_transcript: Path to transcript directory.\n",
        "  \n",
        "  Returns:\n",
        "    Total number of utterances,\n",
        "    Mean number of agent utterances per conversation,\n",
        "    Mean number of caller utterances per conversation,\n",
        "    Total number of words, \n",
        "    Number of unique words \n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "  utt_count = 0\n",
        "  agent_utt_per_chat = []\n",
        "  caller_utt_per_chat = []\n",
        "  total_num_words = 0\n",
        "  unique_words = set()\n",
        "\n",
        "  for file in os.listdir(path_transcript):\n",
        "    with open(os.path.join(path_transcript, file), 'rb') as f:\n",
        "      chat = json.load(f)\n",
        "\n",
        "      utt_in_file = []\n",
        "      agent_utt = 0\n",
        "      caller_utt = 0\n",
        "\n",
        "      for utt in chat:\n",
        "        utt_in_file.append(utt['human_transcript'])\n",
        "        if utt['speaker_role'] == 'agent':\n",
        "          agent_utt +=1\n",
        "        if utt['speaker_role'] == 'caller':\n",
        "          caller_utt += 1\n",
        "        \n",
        "        words = [word for word in word_tokenize(utt['human_transcript']) if word not in [\"[\", \"]\", \"<\", \">\"]]\n",
        "        total_num_words += len(words)\n",
        "        unique_words.update(set(words))\n",
        "        \n",
        "      utt_count += len(utt_in_file)\n",
        "      agent_utt_per_chat.append(agent_utt)\n",
        "      caller_utt_per_chat.append(caller_utt)\n",
        "\n",
        "  return (utt_count, np.mean(agent_utt_per_chat), np.mean(caller_utt_per_chat),\n",
        "          total_num_words, len(unique_words))\n",
        "  #############################   "
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev_0jnfgfmka",
        "outputId": "b5237297-2fda-4112-fac0-a7b476182414"
      },
      "source": [
        "%%time\n",
        "total_num_utt, agent_utt_per_chat, caller_utt_per_chat, total_num_words, num_unique_words = transcript_statistics(transcript_path)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.16 s, sys: 585 ms, total: 6.75 s\n",
            "Wall time: 13min 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfFDW6mStPpi",
        "outputId": "48c244b8-7b1c-434b-d732-cc30967d29af"
      },
      "source": [
        "print('''\n",
        "      Total number of utterances: {}\n",
        "      Mean number of agent utterances per conversation: {}\n",
        "      Mean number of caller utterances per conversation: {}\n",
        "      Total number of words: {}\n",
        "      Number of unique words: {} '''.format(total_num_utt, agent_utt_per_chat, \\\n",
        "                                            caller_utt_per_chat, total_num_words, \\\n",
        "                                            num_unique_words))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Total number of utterances: 25730\n",
            "      Mean number of agent utterances per conversation: 9.195712309820193\n",
            "      Mean number of caller utterances per conversation: 8.59820193637621\n",
            "      Total number of words: 146853\n",
            "      Number of unique words: 736 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNIS-mpiHmMK"
      },
      "source": [
        "##### <ins>**Task 1.2.2**</ins>**: Vocabulary Growth (3 points)**\n",
        "\n",
        "Plot how the vocabulary size (i.e. number of unique words spoken) grows with the number of converations (i.e. transcripts) e.g. with 10\\% of the dataset size, what is the vocabulary size? (`X`: Number of transcripts, `Y`: Number of unique words).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5RydWXJMKz"
      },
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cimtUp3PJRkc"
      },
      "source": [
        "##### <ins>**Task 1.2.3**</ins>**: Distribution of Utterances (2 points)**\n",
        "\n",
        "Plot the distribution of number of utterances per conversation and tell us the average number of utterances per conversation. (`X`: Number of utterances in a transcript, `Y`: Number of transcipts with `x` number of utterances)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1zyb1n0JZgr"
      },
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP_9B4nDSwuJ"
      },
      "source": [
        "##### <ins>**Task 1.2.4**</ins>**: Distribution of Tasks (2 points)**\n",
        "\n",
        "Plot histogram of the distribution of tasks. The metadata files describe the call center scenario for each converstaion. We will look at **tasks** field for further inference. \n",
        "\n",
        "**tasks:** field indicates the customer’s goal/intent in the conversation. There are 8 tasks: _order checks, check balance, replace card, reset password, get branch hours, pay bill, schedule appointment, and transfer money_. (`X`: Eight tasks, `Y`: Number of conversations) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7vVC_qJTqpT"
      },
      "source": [
        "task_names = ['order checks', 'check balance', 'replace card', 'reset password', 'get branch hours', 'pay bill', 'schedule appointment', 'transfer money']\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJRAp11LTfNF"
      },
      "source": [
        "##### <ins>**Task 1.2.5**</ins>**: Box plots for number of words per coversation split by task (3 points)**\n",
        "\n",
        "Create a box plot for number of words per conversation. Create a plot for all data. Then split the data by conversation task/intent to create a separate box plot for each. You should create a total of 9 box plots. Comment on your findings, are some tasks quicker than others?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zha_XcjGSx1Y"
      },
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGzcWYiJSy4e"
      },
      "source": [
        "##### <ins>**Task 1.2.6**</ins>**: Distribution of Dialog Actions (2 points)**\n",
        "\n",
        "Plot histogram the distribution of dialog actions for utterances. Use **\"dialog_acts\"** field from each transcript json files. What can you infer from this plot? Please remove \"gridspace_\" from the labels. (e.g. \"greeting\" instead of \"gridspace_greeting\")\n",
        "\n",
        "(`X`: Dialogue action classes, `Y`: Number of utterances) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_nsJaYZS9yP"
      },
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucjiVjaJPrl-"
      },
      "source": [
        "##### <ins>**Task 1.2.7**</ins>**: Distribution of Emotions (2 points)**\n",
        "\n",
        "Plot boxplots that show distribution of probabilities for each emotion category across utterances. Use **\"emotion\"** field from each transcript json files. What can you infer from this plot? (`X`: Emotion Classes, `Y`: Probability) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz4W3TJpPrAl"
      },
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFKjz0Ytsn3a"
      },
      "source": [
        "# Part 2: Implementing CTC Loss\n",
        "\n",
        "We introduce the Connectionist Temporal Classification (CTC) objective, which is a popular objective used to train neural networks to do speech recognition. Critically, it does not require you to know the alignments between inputs and outputs.\n",
        "\n",
        "#### **Summary of CTC**\n",
        "\n",
        "We highlight some of the main features of the CTC objective.\n",
        "\n",
        "- Given a sequence of inputs `x_1, x_2, .., x_T`, an ASR model will map each of these to a probability of an alphabet of C tokens: `p_1, p_2, ..., p_T`. For example, if we are decoding to three possible characters, the probabilities for timestep `t` could look like `p_t = [0.3, 0.2, 0.5]`.\n",
        "\n",
        "- We also have a sequence of symbol targets `y_1, y_2, ..., y_S` (note `S <= T`). Each `y_s` is a number from `0` to `C-1` (e.g. one of the characters).\n",
        "\n",
        "- The main strategy is to decode each `x_t` to a predicted character `y'_t`. For example, `x_1, x_2, ..., x_6 -> c c a a a t`. This mapping is called an **alignment**. Then we can collapse repetitions to get `c a t` as the predicted output sequence. This simple strategy has some problems: how do you handle silences or repeated characters?\n",
        "\n",
        "- CTC adds a special `blank` token. We'll call this `eps`. Now consider this example, `x_1, x_2, ..., x_12 -> h h e eps eps l l l eps l l o`. Now we can collapse everything in between each `eps` to get `h e eps l eps l o`. Removing blank tokens, we get the prediction `hello`.\n",
        "\n",
        "- For an output sequence, there are many \"valid\" alignments. For example, given an input sequence `x_1, ..., x_6` and an output sequence `c a t`, valid alignments include `eps c c eps a t`, `c c a a t t`, or `c a eps eps eps t`. Example of invalid alignments include `c eps c eps a t`, `c c a a t` (too short if the input sequence has 6 tokens), and `c eps eps eps t t`.\n",
        "\n",
        "- Let `A` represent all valid alignments of an output sequence to an input sequence. A simplified pseudocode for the CTC objective might look like:\n",
        "\n",
        "```\n",
        "all_log_prob = 0\n",
        "\n",
        "for each (a_1, a_2, ..., a_S) in A:\n",
        "\n",
        "    log_prob = 0\n",
        "\n",
        "    for t in 1 to T:\n",
        "        # The alignment a_s has a specific output character for input position t\n",
        "        log_prob_t = log p(a_(s,t) | x_1, ..., x_T)\n",
        "        # compute the joint probability by multiplying independent time steps\n",
        "        # Adding in log space to avoid underflow\n",
        "        log_prob += log_prob_t\n",
        "\n",
        "    all_log_prob += log_prob\n",
        "```\n",
        "That is, the CTC loss computes the probability of all possible alignments. Please note that the pseudocode above is for intuition. In practice, it is often too slow to enumerate over `A` explicitly. \n",
        "\n",
        "As an example, for an output sequence of length 50 (without any repeated characters) and an input sequence of length 100, the number of unique alignments is almost 10^40. \n",
        "\n",
        "#### **Dynamic Programming in CTC**\n",
        "\n",
        "In class, we introduced the \"forward algorithm\" to tractably compute likelihoods for HMMs. We can do something similar to score alignments more efficiently than manually enumerating over the full set.\n",
        "\n",
        "In other words, we can do dynamic programming. Since many alignments share partial sub-sequences, we can  store the `log_prob` for all sub-sequences we have seen so far. This allows us to reuse computation when computing the likelihood for a new sequence.\n",
        "\n",
        "The logic is as follows:\n",
        "\n",
        "Recall `X` is the input sequence of maximum length `T` and `Y` is the output sequence of maximum length `S`. Build a new sequence `Z` that adds a blank token between every character. \n",
        "\n",
        "```\n",
        "Y = [y_1, y_2, ..., y_S]\n",
        "Z = [eps, y_1, eps, y_2, ..., eps, y_S, eps]\n",
        "  = [z_1, z_2, z_3, z_4, ..., z_2S-1, z_2S, z_2S+1]\n",
        "```\n",
        "Note that the length of `Z` is now `2S+1`.\n",
        "\n",
        "**Step 1: Make a Cache.** \n",
        "\n",
        "Instantiate a matrix of size `T x (2S+1)`. Call this matrix `C`. The index `C[t][s]` represents a probability score for the subsequence `z_1, ..., z_s` after observing `x_1, ..., x_t`. That is, `C[t][s] = p(y_1, ..., y_s/2 | x_1, ..., x_t)`.\n",
        "\n",
        "**Step 2: Make an Update Rule.**\n",
        "\n",
        "The goal of dynamic programming is to reuse `C[t-1]` in computing `C[t]`. To do that, we need to define an update rule. There are two cases depending on what  the token `z_s` is.\n",
        "\n",
        "- <ins>Case 1</ins>: `z_s` is a blank token OR `z_s = z_s-2`. This is the standard forward algorithm update. We build `p(z_1, ..., z_s|x_1, ..., x_t)` using two parts: `p(z_1, ..., z_s|x_1, ..., x_t-1)` and `p(z_1, ..., z_s-1|x_1, ..., x_t-1)`. Assuming an increasing order of `t` and `s`, these two parts will already have been computed. (If `s = 0` you can ignore the `C[t-1][s-1]` term. If `t = 0`, set `C[t][s] = p(z_s | x_1, ..., x_t)`.)\n",
        "```\n",
        "C[t][s] = (C[t-1][s-1] + C[t-1][s]) * p(z_s | x_1, ..., x_t)\n",
        "```\n",
        "In other words, `C[t-1][s-1]` and `C[t-1][s]` are known. The prediction for `p(z_s | x_1, ..., x_t)` is known (e.g. `log_probs`).\n",
        "\n",
        "- <ins>Case 2</ins>: `z_s` is not a blank token AND `z_s != z_s-2`. The tricky part is to notice that `z_s-1` is a blank token. Since our Markov assumption says `y_t` should depend on `y_t-1`, which means `z_s` should depend on both `z_s-1` and `z_s-2` (since we artificially added `z_s-1`). Similarly, ignore terms that do not exist.\n",
        "```\n",
        "C[t][s] = (C[t-1][s-2] + C[t-1][s-1] + C[t-1][s]) * p(z_s | x_1, ..., x_t)\n",
        "```\n",
        "\n",
        "Now, we still need two for loops to loop over `1 ... T` and `1 ... S` but this is usually much cheaper than looping over alignments.\n",
        "\n",
        "```\n",
        "C = init_cache(T, S)\n",
        "for t in 1 to T:\n",
        "  for s in 1 to 2S+1:\n",
        "    C = do_update(t, s, C)\n",
        "\n",
        "p_y_given_x = C[-1][-1] + C[-1][-2]  # sum the probability of the last epsilon and last non-epsilon tokens\n",
        "```\n",
        "\n",
        "For more information, refer to this [blog](https://distill.pub/2017/ctc/) or the original [paper](https://www.cs.toronto.edu/~graves/icml_2006.pdf) by Alex Graves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBsN5YSz98CA"
      },
      "source": [
        "### **Step 2.1: CTC Objective [15 Points]**\n",
        "\n",
        "Please write a function in PyTorch that given a minibatch of model predicted probabilities and a minibatch of output sequences, computes the CTC objective.\n",
        "\n",
        "**Note:** You cannot use the built-in `F.ctc_loss` in PyTorch. We will use this fast library for experiments in HW4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zSVCxbF-ydG"
      },
      "source": [
        "def ctc_loss(\n",
        "    log_probs: torch.FloatTensor, targets: torch.LongTensor,\n",
        "    input_lengths: torch.LongTensor, target_lengths: torch.LongTensor,\n",
        "    blank: int = 0) -> torch.Tensor:\n",
        "  \"\"\"Connectionist Temporal Classification implementation.\n",
        "\n",
        "  Args:\n",
        "    log_probs: The log-beliefs returned by an ASR model.\n",
        "      This is `log p(a_t | x_1, ..., x_T)`.\n",
        "      (Shape: T x batch_size x C, where T is a maximum input sequence length and\n",
        "      C is the alphabet size (including blank))\n",
        "\n",
        "    targets: Sequence of contiguous output labels (no blanks).\n",
        "      This is `y_1, ..., y_S`.\n",
        "      (Shape: batch_size x S, where S is a maximum output sequence length)\n",
        "\n",
        "    input_lengths: Lengths of each example in minibatch (<= T).\n",
        "      (Shape: batch_size)\n",
        "\n",
        "    target_lengths: Lengths of each target in minibatch (<= S).\n",
        "      (Shape: batch_size)\n",
        "\n",
        "    blank: The \"epsilon\" token that is used to represent silence.\n",
        "      (integer <= C, default 0)\n",
        "\n",
        "  Returns:\n",
        "    CTC loss averaged over minibatch.\n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "  pass\n",
        "  #############################\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jB1nmtAptMT"
      },
      "source": [
        "Here is a good sanity check. Test you code by checking below that `test` and `soln` are roughly equal. It's okay if your solution is much slower since the Pytorch one is coded in C. However, we will deduct points if you enumerate over all alignments as this is too slow for practical use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxFcWI49psc_"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n",
        "targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n",
        "input_lengths = torch.full((16,), 50, dtype=torch.long)\n",
        "target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n",
        "\n",
        "est = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0)\n",
        "soln = torch.mean(F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='none'))\n",
        "\n",
        "print(est.detach().numpy(), soln.detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn5E7hwBhL8j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baSDEXtDqm59"
      },
      "source": [
        "##### <ins>**Task 2.1.1**</ins>**: Demonstrate your implementation [15 points]**\n",
        "\n",
        "Print your loss function results on the log_probs and targets we provide. You can load the test minibatches by calling `get_test_minibatches()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0gCS7qn3GhB"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "def get_test_minibatches() -> list:\n",
        "  \"\"\"Get minibatches to test implementation\n",
        "  Returns:\n",
        "    lists of log_probs, targets, input_lengths, target_lengths\n",
        "  \"\"\"\n",
        "  torch.manual_seed(224)\n",
        "  np.random.seed(224)\n",
        "  random.seed(224)\n",
        "\n",
        "  T = np.array([])\n",
        "  C = np.array([])\n",
        "  N = np.array([])  \n",
        "  S = 40  # Target sequence length of longest target in batch (padding length)\n",
        "  for i in range(10):\n",
        "    T = np.append(T, random.randint(i+50, i+80))\n",
        "    C = np.append(C, random.randint(int(0.4* T[i]), int(0.8*T[i])))\n",
        "    N = np.append(N, random.randint(int(0.7 *C[i]), int(0.9*C[i])))\n",
        "      \n",
        "  log_probs = []\n",
        "  input_lengths = []\n",
        "  target_lengths = [] # 10 x N[i]\n",
        "  targets = []\n",
        "  for i in range(10):\n",
        "    log_probs.append(torch.randn(int(T[i]), int(N[i]), int(C[i])).log_softmax(2).detach().requires_grad_())\n",
        "    input_lengths.append(torch.full((int(N[i]),), fill_value = int(T[i]), dtype=torch.long))\n",
        "    target_lengths.append(torch.randint(low = 1,high = S, size = (int(N[i]),), dtype=torch.long))\n",
        "    targets.append(torch.randint(low = 1, high = int(C[i]), size = (int(N[i]), S), dtype=torch.long))\n",
        "\n",
        "  return log_probs, targets, input_lengths, target_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT0wjqLZ5Y6O"
      },
      "source": [
        "log_probs, targets, input_lengths, target_lengths = get_test_minibatches()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeFS6QdamUWw"
      },
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOya4Tu2p-vF"
      },
      "source": [
        "### Step 2.2: CTC Decoding [15 Points]\n",
        "\n",
        "Given a sequence of log probabilities ` p_1, ..., p_T,` we often want to decode them to the most likely output sequence `y'_1, y'_2, ..., y'_S`. Ideally, we do this exactly using the Forward-Backward algorithm, but this could be computationally infeasible. A popular thing to do in practice is to use beam search to approximate exact inference. \n",
        "\n",
        "Below, you will write a function to decode log probabilities (from a \"CTC model\") to sequences by beam search. \n",
        "\n",
        "For context: to find the maximum likelihood decoding, taking `argmax` at each timestep is not sufficient. Even though CTC assumes conditional independence between characters, greedy decoding does not take into account that multiple output sequences can collapse to the same post-collapsed transcription.\n",
        "\n",
        "As an example, suppose the pre-collapsed sequences `a a epsilon` and `a a a` are each less likely than the sequence `b b b`. Greedy decoding would pick `b b b` as the most likely sequence. However, since `a a epsilon` and `a a a` collapse to the same string `a`, we can sum their probabilities, which could be more probable than `b b b`. These kind of edge cases make beam search a more effective method for decoding CTC, even without adding a language model.\n",
        "\n",
        "We strongly recommend you read this [blog](https://distill.pub/2017/ctc), which describes how to do beam search for CTC in detail. As a hint, unlike standard beam search, after choosing the top candidates at every stage, we have to sum the probability for candidates that collapse to the same sequence. This requires you to carefully handle blank tokens (`epsilon`) and repeated tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv3a8QReqoMy"
      },
      "source": [
        "import torch\n",
        "\n",
        "def beam_search(log_probs: torch.LongTensor, beam_width: int = 5) -> list:\n",
        "  \"\"\"Beam search to find the most likely sequence.\n",
        "  \n",
        "  You can assume the inputs are not in a minibatch (just a single example).\n",
        "  You can also assume the blank token is index 0.\n",
        "\n",
        "  Args:\n",
        "    log_probs: Log probabilities as defined in CTC. (Shape: T x C)\n",
        "    beam_width: Number of candidates to keep around.\n",
        "                      \n",
        "  Returns:\n",
        "    outputs: [y'_1, y'_2, ..., y'_T], where each y'_t is between 0 and C-1.\n",
        "             This represents the most likely sequence found by beam search.\n",
        "      (shape: T) \n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "  pass\n",
        "  #############################\n",
        "\n",
        "\n",
        "def ctc_decode(\n",
        "    log_probs: torch.LongTensor, blank: int = 0, beam_width: int = 5) -> list:\n",
        "  \"\"\"Decoding with CTC.\n",
        "\n",
        "  Use beam search to approximate the maximum likelihood decoding\n",
        "  from `log_probs`. Make sure that blank tokens are removed afterwards\n",
        "  and unnecessary repeated tokens are removed as well.\n",
        "\n",
        "  Args:\n",
        "    log_probs: log probabilities as defined in CTC. (shape: T x C)\n",
        "    blank: The \"epsilon\" token that is used to represent silence.\n",
        "      (integer <= C, default 0)\n",
        "    beam_width: The number of candidates to keep around.\n",
        "\n",
        "  Returns:\n",
        "    outputs: [y'_1, y'_2, ..., y'_S], where each y'_t is between 0 and C-1.\n",
        "      (shape: S (!= T))\n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "  pass\n",
        "  #############################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmgpwAxnlIVz"
      },
      "source": [
        "##### <ins>**Task 2.2.1**</ins>**: Demonstrate beam search decoding [10 points]**\n",
        "\n",
        "Print the most likely transcript for each `log_probs` and character set we provide. Use the default beam width (=5). You can load the test data  by calling `get_log_probs()`. Please loop through  `log_probs_batch` to get `log_probs` input to test your beam search implementation. You will output 10 likely transcripts by using the test `log_probs` in `log_probs_batch`.\n",
        "\n",
        "**Note:** The most likely transcript could be gibberish. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwxNRPEKDFTn"
      },
      "source": [
        "def get_log_probs() -> torch.LongTensor:\n",
        "  \"\"\"Get minibatches to test implementation\n",
        "  Returns:\n",
        "    lists of log_probs\n",
        "  \"\"\"\n",
        "  torch.manual_seed(224)\n",
        "  np.random.seed(224)\n",
        "  random.seed(224)\n",
        "\n",
        "  T = 50 # input length\n",
        "  N = 10 # Batch size\n",
        "  C = 27 # Class size\n",
        "\n",
        "  # \"CTC model\" probabilities\n",
        "  log_probs_batch = torch.randn(N, T, C).log_softmax(2).detach().requires_grad_()\n",
        "  return log_probs_batch \n",
        "\n",
        "log_probs_batch = get_log_probs() # Shape: N x T x C (10, 50, 27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vHav8Z2DUKJ"
      },
      "source": [
        "import string\n",
        "log_probs_list = get_log_probs()\n",
        "char_set = list(string.ascii_lowercase) # lowercase alphabet\n",
        "char_set.insert(0, \"eps\") # add blank as the first element"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7fs0uiFmW5o"
      },
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj5X-BYIluef"
      },
      "source": [
        "##### <ins>**Task 2.2.2**</ins>**: Demonstrate narrowed beam search [5 points]**\n",
        "\n",
        "Print the most likely transcript for each `log_probs` and character set we provide. Use a narrow beam (=2) and comment on any difference you see with the narrow beam as compared to using the default beam size above. Do you find the same sequences? You can load the test data by calling `get_log_probs()`. Please loop through  `log_probs_batch` to get `log_probs` input to test your implementation. You will output 10 likely transcripts by using the test `log_probs` in `log_probs_batch`.\n",
        "\n",
        "**Note:** The most likely transcript could be gibberish. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCiNbnxaIBaJ"
      },
      "source": [
        "import string\n",
        "log_probs_list = get_log_probs()\n",
        "char_set = list(string.ascii_lowercase) # lowercase alphabet\n",
        "char_set.insert(0, \"eps\") # add blank as the first element\n",
        "print(char_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esFCu-4dmZ-z"
      },
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "pass\n",
        "############################# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve2yJL3BiPK5"
      },
      "source": [
        "This is the end of Part 1 & 2. Great work! "
      ]
    }
  ]
}